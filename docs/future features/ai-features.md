Great — I’ll start by researching how to implement an AI-driven quizmaster slide type using OpenAI's API (with function calling), streaming via Vercel SDK, chat limits with Stripe-based upgrades, and decorative AI-generated backgrounds using the latest DALL·E image generation features.

I’ll let you know as soon as I have a clear, implementable guide tailored for this setup.


# AI Quizmaster Slide in a Next.js Onboarding Wizard

## Overview of the AI Quizmaster Slide

In this onboarding slide type, users engage in a **chat-based quiz** with an AI agent. Each slide is backed by three pieces of JSON-defined data:

* **Knowledge Base** – A blob of factual content or text that the quiz covers (the material the user is being tested on).
* **Agent Personality** – A description of the AI’s persona (e.g. a strict quizmaster, friendly coach, etc.) to shape its tone and behavior.
* **Task/Challenge** – A description of the quiz task or the set of questions/challenges the agent should pose to the user.

Using these inputs, the AI acts as a quizmaster: it asks the user questions based on the **knowledge base**, evaluates answers, and gives feedback. The twist is that the AI should not just recite fixed questions – it can leverage the LLM to **generate slight variations** each session (to keep the quiz dynamic), while still adhering to the fixed knowledge and task parameters. The agent is *not* doing open-ended tutoring; it strictly quizzes the user on given content. It should also decide when the user has successfully passed the quiz and then signal completion via a function call.

Key features to implement:

* **Real-time streaming chat:** The quiz conversation should stream token-by-token for immediacy (using OpenAI’s streaming chat API and Vercel’s AI SDK for Next.js).
* **Function calling:** The assistant will use OpenAI’s function calling feature to trigger a backend action (marking the quiz as passed) when appropriate.
* **User message rate limiting:** Each user can only send a certain number of messages per day (e.g. 100/day on free plan), tracked in Postgres. Higher limits or unlimited messages are unlocked via a Stripe-paid plan.
* **DALL·E background generation:** The slide can display a decorative background image generated by OpenAI’s latest image model (e.g. DALL·E 3) based on a user’s prompt, constrained to a specific style (such as the viral Studio Ghibli-like style).
* **Admin configuration:** Administrators need a way to create and edit the slide content (knowledge base, personality, tasks) in the backend, so the system can easily update quiz material without code changes.

In the following sections, we’ll explore each of these components, providing implementation strategies, code snippets, and references to official docs and examples.

## Streaming Chat Implementation (Next.js + Vercel AI SDK)

To create a smooth chat experience, the AI’s responses should stream in real-time as the model generates text. Next.js (with the App Router) combined with Vercel’s AI SDK makes it straightforward to implement streaming chat. The OpenAI Chat Completion API supports server-sent events (SSE) streaming of tokens, and the Vercel AI SDK provides utilities to pipe this stream directly to the client.

**Serverless API Route for Chat:** In your Next.js app (App Router), you can create an API route (e.g. `app/api/quiz/route.ts`) that handles chat requests. This route will:

1. Read the incoming user message (and perhaps conversation history or context) from the request.
2. Construct a chat completion request to OpenAI with streaming enabled.
3. Use the Vercel AI SDK to transform OpenAI’s stream into a web response that the client can consume incrementally.

For example, using the Vercel AI SDK’s `OpenAIStream` helper and Next.js Edge runtime:

```typescript
// app/api/quiz/route.ts
import { OpenAIStream, StreamingTextResponse } from 'ai';
import OpenAI from 'openai';

export const config = { runtime: "edge" };  // Use Edge for low latency streaming

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

export async function POST(request: Request) {
  const { userMessage, context } = await request.json(); 
  // Build the messages array with system prompt from context and user message
  const messages = [
    { role: "system", content: context.systemPrompt },   // includes persona, instructions
    { role: "assistant", content: context.knowledgeBase },   // provide knowledge (could also split into system role)
    { role: "user", content: userMessage }
  ];

  // Request OpenAI chat completion with streaming
  const response = await openai.chat.completions.create({
    model: "gpt-4", 
    messages,
    stream: true,                      // enable streaming
    functions: context.functions || [] // (we will set up function definitions later)
  });
  
  // Convert the OpenAI response into a streaming text response for Next.js
  const stream = OpenAIStream(response);
  return new StreamingTextResponse(stream);
}
```

In this snippet, we create an `OpenAIStream` from the OpenAI response and return a `StreamingTextResponse` – this is exactly how Vercel’s AI SDK streams tokens back to the client. Under the hood, it uses web streams (via the Fetch API) to push data as it arrives, which Next.js can send to the browser in real-time.

**Client-Side Streaming Consumption:** On the frontend (the slide component), use either the Vercel AI SDK’s React hooks or manual fetch+stream reading to display messages as they stream in. Vercel’s SDK provides hooks like `useChat`/`useAssistant` that manage the streaming connection for you. For example, using a hook:

```jsx
// In a React Client Component for the quiz slide
"use client";
import { useChat } from 'ai/react';  // hypothetical hook from AI SDK

export default function QuizChat({ slideContext }) {
  const { messages, handleInputChange, input, handleSubmit } = useChat({
    api: '/api/quiz', 
    initialMessages: [
      { role: 'system', content: slideContext.systemPrompt },
      { role: 'assistant', content: slideContext.knowledgeBase }
    ]
  });

  return (
    <div className="quiz-chat">
      <div className="messages">
        {messages.map(m => (
          <p key={m.id} className={m.role}>{m.content}</p>
        ))}
      </div>
      <form onSubmit={handleSubmit}>
        <input 
          value={input} 
          onChange={handleInputChange}
          placeholder="Type your answer..." 
        />
      </form>
    </div>
  );
}
```

In this example, the `useChat` hook takes care of calling our `/api/quiz` endpoint, sending the user’s message, and handling the SSE stream of the assistant’s response. As tokens stream in, `messages` state updates and the UI re-renders incrementally to show the typing effect. This matches the pattern from Vercel’s Next.js AI chatbot templates.

**Ensuring Streaming Works:** A few important notes:

* Use **Edge runtime** or Node’s built-in streams; by default Next.js App Router supports streaming responses. The above config `{ runtime: "edge" }` and using `Response` streams ensures low latency.
* Set the appropriate headers (if not using the SDK helpers) for an SSE response (`Content-Type: text/event-stream` and `Transfer-Encoding: chunked`). The Vercel SDK’s `StreamingTextResponse` handles this for us.
* If not using the AI SDK, you can manually implement streaming by reading the `Response.body` with a `reader` and pushing chunks to the client. However, the SDK abstracts these details and is recommended for simplicity.

By leveraging streaming, the quiz feels interactive and responsive, as the user sees the AI “typing” out questions or feedback in real-time rather than waiting for an entire answer to be generated. The Vercel AI SDK was designed to *“easily stream API responses from AI models”*, which fits our use case perfectly.

## Prompt Design: Injecting Knowledge, Personality, and Tasks

The behavior of the AI quizmaster is driven by how we construct the prompt (the `messages` we send to OpenAI). We need to inject the slide’s configured **knowledge base**, **personality**, and **task** into the prompt so that the assistant knows the material and how to conduct the quiz.

A good strategy is to use **system messages** and/or **assistant messages** at the start of the conversation:

* **System Role (Persona & Instructions):** Compose a system message that establishes the AI’s role and personality, and gives it explicit instructions on how to conduct the quiz. For example: *“You are a strict but helpful quizmaster AI. Your job is to test the user’s knowledge on the given content. Only ask one question at a time from the provided material. Do not give away the answer. Encourage the user to think if they are unsure. When the user has answered all questions correctly, call the `markTestPassed` function.”* This ensures the AI maintains the desired tone and behavior throughout. You can include in this system prompt any rules, such as not going off-topic beyond the knowledge base, or how many tries the user gets.
* **Assistant Role (Knowledge Base):** We want the model to have access to the factual content it should quiz the user on. One approach is to supply the entire knowledge blob as an initial **assistant message** (or as part of the system message). For example, you can add: *Assistant: \[Full text of knowledge base]*. By doing this, we effectively *prime* the model with the study material. The model will treat this as content it “said” before, which it can then formulate questions about. Alternatively, you could put the knowledge base in the system prompt as well (if it’s shorter) or even use OpenAI’s new fine-tuning or retrieval features. A simple method, however, is to include the knowledge text verbatim as context.
* **Task as Instruction:** The “task/challenge/questions” field can be used to further instruct the assistant on what to ask. For instance, if the task JSON says something like *“Ask 5 multiple-choice questions covering the main points of the knowledge base”*, you can append that to the system message: *“Your task: {taskDescription}”*. If the task field contains actual questions or topics, you might incorporate those as guidance (e.g., *“Topics to cover: X, Y, Z.”*).

By structuring the initial `messages` this way, every time the user sends an answer, the model has the full context of who it is, what the material is, and what it’s supposed to do. For example, the initial message array might look like:

```js
messages = [
  { role: "system", content: "You are a quizmaster AI. Personality: " + persona + ". Instructions: Only ask questions based on the provided knowledge. " + taskDescription + " When the user has answered correctly, use function call to mark success." },
  { role: "assistant", content: knowledgeBaseText },
  // ... possibly a first assistant question to start the quiz, or we let the model generate it
  { role: "user", content: userAnswer1 },
  { role: "assistant", content: followUpQuestionOrFeedback },
  // ... and so on
];
```

In practice, you might not hard-code a first question; instead, after the initial system and knowledge messages, you send a blank user prompt or a trigger like “BeginQuiz” that causes the assistant to generate the first question. The key is the assistant always has the knowledge base in the conversation history to reference for correctness.

**Maintaining Quiz Focus:** Because the model has a lot of freedom, it’s important our prompts keep it focused:

* Remind the AI in the system prompt that it **must only use the given knowledge**. It should not invent facts outside that blob (to avoid confusing or misleading the user).
* Remind it this is a test, not a lesson. For example: *“Do not provide the answer until the user attempts an answer. If the user is wrong, encourage them to try again or give a hint based on the knowledge.”* This ensures it doesn’t slip into a teaching mode or reveal answers too easily.
* Set a reasonable **temperature** in the API call. A moderate temperature (e.g. 0.3–0.5) will allow some variation in phrasing of questions without going off the rails, whereas a temperature of 0 would make it deterministic (always the same wording). Slight randomness is desirable to keep each session fresh.

With these prompt design choices, the AI quizmaster will behave in a controlled yet dynamic way. It essentially has the slides’s content “in its head” (from the knowledge message) and a fixed persona and goal (from the system message). This aligns with the idea of grounding the LLM with your custom data and instructions, as recommended in OpenAI’s documentation on aligning models with tools and context.

## Function Calling: Marking the Quiz as Passed

OpenAI’s function calling feature allows the model to output a structured **function call** when certain conditions are met, instead of (or in addition to) normal text. We will use this so that the quizmaster AI can **notify our application when the user has passed the test**. For example, after the user answers all required questions correctly, the assistant should “call” a function named `"markTestPassed"` – which our backend will interpret and then hit a real API endpoint or database update.

**Defining the function:** When we send the chat completion request, we include a `functions` array describing the functions the model is allowed to call. In our case, we might define:

```ts
const functions = [{
  name: "markTestPassed",
  description: "Marks that the user has passed the quiz for this slide",
  parameters: {
    type: "object",
    properties: {
      userId: { type: "string", description: "The user's ID" },
      slideId: { type: "string", description: "The quiz slide identifier" }
    },
    required: ["userId", "slideId"]
  }
}];
```

This JSON schema tells the model what the function is and what arguments it should provide. We’ll pass `functions` in the API call (as shown in the earlier code snippet via `context.functions`). We can also include a special instruction in the system prompt that *explicitly directs the model to use that function* when appropriate (the model might do it on its own, but it helps to be clear in the prompt). For example: *“When the user answers all questions correctly, call the function `markTestPassed` with the user’s ID and slide ID.”*

**Capturing and handling the function call:** OpenAI does not actually execute your function; instead, it will return a response indicating the function name and arguments to call. Our server code needs to detect this and then perform the real action (e.g., call an internal API route or update the DB). The OpenAI Node SDK will return a message with `role: "assistant"` and a `function_call` field. If using the fetch API directly, the JSON looks like:

```json
{
  "id": "...",
  "choices": [
    {
      "message": {
        "role": "assistant",
        "content": null,
        "function_call": {
          "name": "markTestPassed",
          "arguments": "{ \"userId\": \"abc123\", \"slideId\": \"quiz1\" }"
        }
      },
      "finish_reason": "function_call"
    }
  ]
}
```

We can check if `choices[0].finish_reason === "function_call"` (or in newer API versions it might appear as a message with role "assistant" and the `function_call` property). If so, we parse the `arguments` JSON and invoke our own backend logic. For instance:

```ts
const result = await openai.chat.completions.create({ /* ... */, functions });
const choice = result.choices[0];
if (choice.finish_reason === "function_call" || choice.message.function_call) {
   const funcCall = choice.message.function_call;
   if (funcCall.name === "markTestPassed") {
      const args = JSON.parse(funcCall.arguments || "{}");
      // Now call our endpoint to mark the quiz passed
      await fetch(`https://your-backend/api/quizPassed`, {
        method: 'POST',
        headers: { 'Authorization': 'Bearer ' + userToken },
        body: JSON.stringify({ userId: args.userId, slideId: args.slideId })
      });
      // We might also respond to the client or do any other handling here
   }
}
```

If using the streaming approach, handling function calls is a bit trickier because the response may stream token by token (including the function call). One simplification is to not stream once you expect a function result – for example, the quizmaster might send normal messages via streaming, but when ready to conclude, it could output the function call all at once (which we handle after the stream). The OpenAI streaming response will include the function call in the event stream as a special message. The Vercel AI SDK’s stream helpers can also detect function messages, but you might choose to do a final, non-streaming request for the last step.

**Instructing the model for function use:** We should ensure the model knows *when* to call `markTestPassed`. This logic lives entirely in the model’s “head” – you can write in the system prompt: *“Do not call markTestPassed until the user has answered all questions correctly. When you do call it, you can stop the conversation after.”* The model will then only trigger it when conditions are met. It’s essentially following our instructions and the conversation flow to decide when the user has passed.

OpenAI’s function calling is reliable for producing structured output when the model decides a function is needed, but remember: *“the feature doesn't actually execute functions on your behalf… the model generates the call and then you use it in your code”*. So robust error checking is prudent – e.g., verify the arguments, ensure the function is indeed the expected one, etc., before trusting and executing it.

By integrating function calling, our AI quizmaster can **signal completion in a robust, machine-readable way**, instead of relying on brittle text parsing (like looking for “Congrats, you passed!” in the assistant’s message). We offload the pass/fail logic to the AI (guided by our prompt), and handle the outcome cleanly in our backend.

## Rate Limiting and Subscription Upgrades (Postgres + Stripe)

To control costs and prevent abuse, we enforce a **daily chat limit** on users. For example, free users might be limited to 100 messages per day. We’ll use a Postgres database to track usage and a Stripe integration to allow users to upgrade their plan for a higher limit. This involves a few parts: counting user messages, restricting API calls if over limit, and integrating Stripe’s subscription webhooks to update user privileges.

**Counting user messages:** A simple approach is to log each chat message or each API call a user makes. For example, have a `chat_usage` table:

```sql
CREATE TABLE chat_usage (
  user_id UUID,
  date DATE,
  count INTEGER,
  PRIMARY KEY (user_id, date)
);
```

Each time the user sends a message, increment the count for that `user_id` and `current_date`. You can do this in a single UPSERT query. For instance (pseudo-code):

```ts
// Pseudo-code for incrementing usage
await db.query(`
  INSERT INTO chat_usage(user_id, date, count) VALUES($1, CURRENT_DATE, 1)
  ON CONFLICT (user_id, date) DO 
    UPDATE SET count = chat_usage.count + 1;
`, [userId]);
```

Before incrementing, check the current count:

```ts
const { count } = await db.query(`
  SELECT count FROM chat_usage 
  WHERE user_id = $1 AND date = CURRENT_DATE;
`, [userId]);
if (count >= dailyLimit) {
  // Block the request
  return res.status(429).json({ error: "Daily chat limit reached. Please upgrade for more." });
}
```

This ensures a user cannot exceed their allowed number of chats per day. The daily limit (`dailyLimit`) will depend on their plan (we’ll get to that next). You might perform this check at the very start of your `/api/quiz` route, before calling OpenAI.

**Enforcing limits in practice:** It’s wise to implement this check as middleware or in the API handler itself. If using an ORM like Prisma, it could be as simple as:

```ts
const usageToday = await prisma.chatUsage.findUnique({ where: { user_id_date: { user_id: userId, date: today } } });
if (usageToday && usageToday.count >= dailyLimit) {
  throw new Error("Rate limit exceeded");
}
```

(where `dailyLimit` is 100 for free users, etc.). This pattern is similar to how one might enforce shorter rate limits (e.g., per minute) by counting records in a time window, but here our window is 1 day.

**Resetting the count:** The above scheme naturally segregates counts by date. Each new day, a new row will be inserted (since `PRIMARY KEY(user_id, date)` will be different). You can periodically delete old records (or keep them for analytics). Alternatively, you could have a single usage row per user with a timestamp and count, but then you’d need a cron job to reset counts daily. Using the date as part of the key is a simple, cron-free solution.

**Stripe subscription integration:** To allow more usage, we integrate Stripe’s subscription system:

* **Plans and pricing:** In Stripe, create a product for your service with multiple price tiers. For example, a “Free” tier (maybe not handled in Stripe, as it’s free by default), and a “Pro” tier at some monthly price. The Pro tier could have a higher daily limit (or effectively unlimited within reasonable bounds).
* **Stripe Checkout:** On the frontend, when a user chooses to upgrade, redirect them to a Stripe Checkout session for the Pro subscription. This involves calling Stripe’s API (via your backend) to create a checkout session URL.
* **Webhook handling:** The crucial part is to listen for Stripe webhook events such as `checkout.session.completed` and `customer.subscription.updated`. When a user successfully subscribes or when their subscription status changes, your backend receives an event. In the webhook handler (e.g. Next.js API route `/api/stripe-webhook`), verify the event’s signature (using Stripe’s secret) and then update the user’s record in Postgres. Typically, you’d update a field in the `users` table like `plan = 'pro'` or store the Stripe subscription ID/status. For example, Vercel’s official SaaS starter updates a Supabase (Postgres) table of `subscriptions` and `users` via Stripe webhooks.

&#x20;**Integration architecture:** The diagram below illustrates how Next.js (Vercel), Postgres (via Supabase), and Stripe interact. Stripe Checkout is initiated from Vercel, and Stripe calls a webhook on your app upon payment success, which then updates the database (e.g., marking the user as subscribed to Pro). This aligns with the Next.js Subscription Payments template’s approach.

Once the user’s plan is updated in the database (say, `user.plan = 'pro'`), your chat API logic can use that to set `dailyLimit` accordingly. For instance:

```ts
const dailyLimit = user.plan === 'pro' ? 1000 : 100; 
```

You can maintain a mapping of plan -> limit. A Pro user might get 1000 messages/day or maybe “unlimited” (you can decide an upper bound for safety).

**Checking the plan in the API:** When a request comes in, authenticate the user (e.g., via NextAuth or your auth system) to get `userId`. Then fetch the user’s plan from DB (or include it in the auth token if using JWT with custom claims). Then determine the `dailyLimit` for that plan, and perform the usage count check as described.

If the limit is reached, you can respond with a 429 or even a 200 with a message in the chat like “You’ve reached your daily limit.” Some apps might choose to let the AI respond with a friendly message about limits, but since we control the UI, a direct error or toast might suffice.

**Stripe Customer and Identity:** Don’t forget to associate the Stripe customer ID with your user (usually done at signup/checkout). Most integrations add a `stripe_customer_id` field to the users table so you know which Stripe customer corresponds to which user. This comes in handy in webhooks (the event will reference the customer or subscription, and you find the user by that ID).

Using Stripe and Postgres in tandem like this is a common SaaS pattern – you have an authoritative record of the user’s subscription in your database updated via webhooks. The Next.js SaaS starter template, for instance, auto-propagates Stripe product and subscription info into Supabase so the app can easily query it. We’re doing a simplified version: we really just need to know the user’s current plan level or message quota.

**Example workflow:** A user on Free tries to send the 101st message of the day:

1. Our `/api/quiz` handler fetches usage = 100, dailyLimit = 100 -> usage >= limit, so it **does not call OpenAI**.
2. It responds with an error or a special message indicating the limit is reached. The frontend can display an upgrade prompt.
3. The user clicks “Upgrade”. We redirect them through Stripe Checkout to subscribe to Pro.
4. They complete payment. Stripe sends a `checkout.session.completed` event -> our `/api/stripe-webhook` handler receives it, finds which user it’s for (using the session’s customer or metadata), and updates `users.plan` to `'pro'` in Postgres, and maybe inserts a row in `subscriptions` table.
5. Now the user’s plan is Pro. When they come back to continue, our API sees `user.plan = 'pro'` and so sets `dailyLimit = 1000`. The usage table still had 100 for today, but now the limit is higher, so the user can continue chatting up to 1000. (Alternatively, you might reset their count upon upgrade, but that’s a product decision.)

By implementing this, we ensure fair use: *free users have a cap, and power users can pay to lift the cap*. The Postgres-based tracking is reliable and can be extended – e.g., you could also enforce a **monthly** total if desired or deduct from a pool of credits. In our example we stick to daily count.

## Generating Ghibli-Style Backgrounds with DALL·E 3

To enhance the slide visually, we allow users to generate a **decorative background image** via the OpenAI image generation API. The user can enter a prompt describing the scene they want (e.g. “a sunny field with mountains in the distance”), and our system will produce an image, stylized in the popular Studio Ghibli-like art trend that recently went viral.

**Using OpenAI’s Image API:** OpenAI’s latest image model (DALL·E 3 as of 2024) is accessible through the API by specifying the model. We can call the images endpoint with the user’s prompt. In Node.js, using OpenAI’s SDK:

```ts
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
const imageResponse = await openai.images.generate({
  model: "dall-e-3",              // use DALL·E 3
  prompt: finalPrompt,           // we'll craft this prompt shortly
  size: "1024x1024", 
  n: 1
});
const imageUrl = imageResponse.data.data[0].url;
```

This would return a URL to the generated image (or a base64 string if requested). Note that DALL·E 3 currently only supports `n=1` (one image per request) for performance reasons. That’s fine for our use case, since we just need one background at a time.

**Prompt style enforcement:** We want the images to have a consistent style (e.g. Ghibli animation style). A good practice is to **augment the user’s prompt** with a predefined style prompt. This acts like a system prompt for image generation. For example, we might prepend something like: *“Illustration in the style of Studio Ghibli, with soft colors and fantastical elements: ”* to whatever prompt the user provides. If the user says “forest with ancient trees”, the final prompt sent to the API becomes *“Illustration in the style of Studio Ghibli, with soft colors and fantastical elements: forest with ancient trees”*. This approach nudges the AI to produce images in the desired style consistently.

OpenAI mentioned that DALL·E 3 expects highly detailed prompts and in fact **auto-embellishes prompts internally**, similar to how ChatGPT does, to include more detail and diversity. Even so, including style keywords explicitly is helpful to lock down the aesthetic. We should also be mindful of the content policy – e.g., avoid using actual artist names or anything disallowed in the prompt.

Additionally, DALL·E 3 introduced a `style` parameter that can be set to `"vivid"` or `"natural"`. The “vivid” style tends to produce more vibrant, illustration-like results (which might align with the Ghibli vibe). We can experiment with this parameter:

```ts
const imageResponse = await openai.images.generate({
  model: "dall-e-3",
  prompt: finalPrompt,
  style: "vivid",
  size: "1024x1024",
  n: 1
});
```

Using `style: "vivid"` can enhance the colors and fantastical quality, whereas `"natural"` would aim for realism. The default is vivid, so even if we omit it, we likely get colorful images.

**Securing the prompt:** To ensure users don’t accidentally (or intentionally) break the format or request something against policy, you might sanitize or constrain their input. Wrapping it in our own style prompt mostly handles that by context, but you should still use the **Moderation API** on user-provided prompts if you want to be extra safe (OpenAI automatically moderates image prompts too, rejecting disallowed content).

**Rendering the image:** Once we get the `imageUrl`, we can set it as the CSS background of the slide. We might store the URL in state so that even if the user leaves and returns, we don’t regenerate unnecessarily (unless we want a fresh image each time). Note that image generation is relatively expensive and slow (hundreds of milliseconds to a few seconds). You might consider generating the image **ahead of time** (e.g., right when the user reaches the slide, or even offline by an admin) to avoid delay during onboarding. But allowing the user to customize it with their own prompt is a nice interactive feature.

For reference, generating an image with the OpenAI API in Node is straightforward – a minimal example:

```js
// Example from OpenAI docs
const response = await openai.createImage({
  prompt: "A cute cat, Studio Ghibli style",
  n: 1,
  size: "512x512"
});
const url = response.data.data[0].url;
```

Our use is just a variant of this, with added style text. Many developers have integrated DALL·E 2 similarly, and the same applies to DALL·E 3 (just remember to specify the model if needed, since it defaults to 2). The result URLs can be used directly in an `<img>` tag or CSS `background-image`.

By incorporating DALL·E, we give users a fun way to personalize their onboarding experience. The “viral Ghibli-style trend” that erupted when DALL·E 3 was introduced shows how engaging these stylized AI images can be. With our system prompt wrapper, we ensure the outputs stay on-theme and format, no matter what the user asks for. (For instance, if a user entered a very minimal prompt, the system addition “Studio Ghibli style” will still push the result toward a beautiful illustration.)

**Note:** Generating images on the fly does come with cost and time considerations. You might implement caching – e.g., if the same user prompt is used often, cache the image URL or upload it to your own storage. Also, because this is an onboarding wizard, one could pre-generate a few default backgrounds to choose from, to avoid calling the API for every single new user unless they want something custom. But as per the spec, we want to let the user generate via their own prompt, so just be mindful of handling slow generation (show a loading spinner, etc., as the image is being created).

## Admin Configuration of Quiz Slides

The content of each quiz slide (knowledge base, personality, task JSON) will likely evolve over time, so we need a convenient way for admins to update it **without redeploying code**. There are a couple of strategies to enable this:

* **Store slide data in the database:** The simplest approach is to have a `quiz_slides` table in Postgres that stores these JSON fields. For example:

  ```sql
  CREATE TABLE quiz_slides (
    id SERIAL PRIMARY KEY,
    title TEXT,
    knowledge_base TEXT,     -- or JSONB if you want to store structured content
    personality TEXT,
    task TEXT,
    created_at TIMESTAMP,
    updated_at TIMESTAMP
  );
  ```

  The `task` could be a JSON field if it’s complex (e.g., a list of questions), or just a description string. The `knowledge_base` might be large, so `TEXT` or `JSONB` works. Admins can then insert or update rows in this table to change the quiz. This could be done through a simple admin panel or even a direct DB admin interface.

* **Admin UI:** Build a protected admin page in Next.js where an authorized admin can edit the fields. This could use a form with a rich text editor for the knowledge base (if it’s large text) and simple inputs for personality and task. When the admin saves, you’d call an API route (e.g. `POST /api/admin/updateSlide`) that writes to the `quiz_slides` table. Ensure this API is secured (check `req.user` role or an admin secret, etc.) because you don’t want normal users hitting it.

* **Using a Headless CMS:** As an alternative, a non-developer-friendly option is to use a CMS like Contentful, Strapi, or even Markdown files in a Git repo. But given we already have a database and want tight integration, storing in Postgres is straightforward. You can then even version or draft content as needed.

**Loading the slide config:** When a user reaches the quiz slide in the onboarding wizard, your app needs to fetch the config. If it’s in Postgres, you could either:

* Query it server-side in a Next.js server component or via `getServerSideProps`/API call. For instance, use your DB client to fetch the slide by ID.
* Or fetch it on the client side by calling a dedicated endpoint like `/api/slides?slideId=123`. The endpoint queries the DB and returns the JSON.

Using server components (if on App Router) is neat because you can directly load the data and pass it to the client component. For example:

```tsx
// app/(onboarding)/quiz-slide/page.tsx - Next.js server component
import { db } from '@/lib/db';
export default async function QuizPage({ params }) {
  const slide = await db.query('SELECT * FROM quiz_slides WHERE id = $1', [params.id]);
  // pass slide to a client component as props or use in JSX
  return <QuizSlideClient slide={slide} />;
}
```

However, in many cases an API route or using a data fetching hook on the client is also fine.

**Example admin update flow:**

* The admin navigates to an Admin UI page listing all slides. They choose “Edit” on the quiz slide.
* They see a form with three fields: *Knowledge Base* (perhaps a textarea), *Personality* (text), *Task/Challenge* (text).
* They make changes and hit Save.
* This triggers a `fetch('/api/admin/saveSlide', { method: 'POST', body: JSON.stringify(updatedFields) })`.
* The API route authenticates the admin (maybe using NextAuth to check `session.user.isAdmin`).
* It then does an `UPDATE quiz_slides SET knowledge_base=$1, personality=$2, task=$3, updated_at=NOW() WHERE id=$4`.
* Returns success. Our app could revalidate any cached data (if using SWR or React Query on client, or just refetch).

This way, non-developers can tweak quiz content easily – for example, if a correction in the knowledge base is needed or they want to change the agent’s tone, they can do so from the admin UI.

**On the fly vs precomputed variations:** Since the AI will generate question variations, the admin doesn’t necessarily need to input many variations of each question – just the general task or a representative question. The model will do the rest. But if you wanted, the task JSON could even include a list of question/answer pairs for reference. You might then feed those into the prompt as examples (few-shot learning). That would make the quiz more deterministic. However, the problem statement suggests leveraging the LLM’s capabilities for variation, so probably no need for the admin to provide a full question bank – just the core info and guidelines.

**Admin testing:** It could be useful to allow the admin to test the quiz with the AI before making it live (especially if using few-shot or specific prompts). Perhaps have a toggle to run the slide in “test mode” where the admin can simulate being a user and see the AI’s questions. This could be as simple as a hidden route that loads the slide and allows conversation (basically the same as the user experience, but not in the normal onboarding flow). This way, admins can fine-tune the persona and task instructions by trial and error.

Finally, ensure your admin UI/actions are **secured**. If using NextAuth, mark the admin pages with `useSession` and redirect if not admin. For API routes, you can create a middleware or just check inside the handler. This prevents unauthorized tampering of slide content.

## Putting It All Together

With all these pieces implemented, our Next.js onboarding wizard gains a rich, interactive quiz slide:

* The AI quizmaster streams questions and responses smoothly to the user in real-time, powered by Next.js Edge functions and the Vercel AI SDK.
* It uses the provided knowledge base and follows the configured personality/task, ensuring the quiz is on-point and consistent each session.
* Once the user demonstrates mastery, the AI triggers a backend function call to mark the test as passed, which our system handles to record progress.
* The application enforces daily usage limits via Postgres, and the integration with Stripe Checkout allows those limits to be increased for paying users – with Stripe webhooks keeping our database in sync with subscription status.
* Users can generate a beautiful background image for the slide via OpenAI’s image API, with a system-crafted prompt to maintain a Studio Ghibli-inspired style across all user-generated images.
* Administrators have control over the quiz content through a backend interface, storing JSON in Postgres for easy updates, ensuring the system can evolve (new questions or topics) without code changes.

By prioritizing official SDKs and best practices, we ensure the solution is robust and maintainable. The Vercel AI SDK and OpenAI APIs handle the heavy lifting of streaming and AI generation, while Postgres and Stripe provide a reliable backbone for user management and monetization. This design is aligned with how real-world SaaS applications incorporate AI: ephemeral generation on the frontend with solid persistence and business logic on the backend.

**References:**

* OpenAI Streaming via Vercel AI SDK (Next.js example)
* Vercel AI SDK documentation on streaming & React hooks
* OpenAI Function Calling explanation
* Vellum AI tutorial on function call usage (ensuring structured output)
* Pedro Alonso’s guide (pre-paid credits) illustrating rate limit logic in Prisma (analogy for our daily limit)
* Vercel’s Next.js Subscription Payments starter, showing Stripe->Supabase integration and plan syncing
* OpenAI Help Center on DALL·E 3 API (prompt handling and style param)
* Studio Ghibli style AI art trend coverage (for context on image style)
